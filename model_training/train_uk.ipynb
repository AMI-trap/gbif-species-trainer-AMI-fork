{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author       : Aditya Jain\n",
    "Date Started : June 14, 2022\n",
    "About        : This is the main training file for training the uk moth classifier\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torchvision.models as torchmodels\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import webdataset as wds\n",
    "import sys \n",
    "\n",
    "from data import dataloader\n",
    "from models.build_model import build_model\n",
    "from data.mothdataset import MOTHDataset\n",
    "from training_params.loss import Loss\n",
    "from training_params.optimizer import Optimizer\n",
    "from evaluation.micro_accuracy_batch import MicroAccuracyBatch\n",
    "from evaluation.micro_accuracy_batch import add_batch_microacc, final_microacc\n",
    "from evaluation.macro_accuracy_batch import MacroAccuracyBatch\n",
    "from evaluation.macro_accuracy_batch import add_batch_macroacc, final_macroacc, taxon_accuracy\n",
    "from evaluation.confusion_matrix_data import confusion_matrix_data\n",
    "from evaluation.confusion_data_conversion import ConfusionDataConvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload files in `my_local_module`\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"note\": \"This config file is for training on the complete macromoths dataset\",\n",
      "   \"model\": {\n",
      "      \"species_num_classes\": 3175,\n",
      "      \"genus_num_classes\": 1088,\n",
      "      \"family_num_classes\": 71,\n",
      "      \"type\": \"efficientnetv2-b3\",\n",
      "      \"preprocess_mode\": \"tf\"\n",
      "   },\n",
      "   \"dataset\": {\n",
      "      \"label_info\": \"/home/mila/a/aditya.jain/gbif_species_trainer/model_training/data/uk-denmark_numeric_labels.json\",\n",
      "      \"taxon_hierarchy\": \"/home/mila/a/aditya.jain/gbif_species_trainer/model_training/data/uk-denmark_taxon_hierarchy.json\"\n",
      "   },\n",
      "   \"training\": {\n",
      "      \"batch_size\": 64,\n",
      "      \"image_resize\": 300,\n",
      "      \"epochs\": 30,\n",
      "      \"early_stopping\": 4,\n",
      "      \"start_val_loss\": 100000000,\n",
      "      \"loss\": {\n",
      "         \"name\": \"crossentropy\"\n",
      "      },\n",
      "      \"optimizer\": {\n",
      "         \"name\": \"sgd\",\n",
      "         \"learning_rate\": 0.001,\n",
      "         \"momentum\": 0.9\n",
      "      },\n",
      "      \"wandb\": {\n",
      "         \"entity\": \"moth-ai\",\n",
      "         \"project\": \"UK-Denmark-Moth-Classifier\"\n",
      "      },\n",
      "      \"model_save_path\": \"/home/mila/a/aditya.jain/logs/\",\n",
      "      \"model_name\": \"uk-denmark-moth-model\",\n",
      "      \"version\": \"v01\"\n",
      "   }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config_file   = 'config/01-config_uk-denmark.json' #**** This file path should come through command line argument\n",
    "\n",
    "f             = open(config_file)\n",
    "config_data   = json.load(f)\n",
    "print(json.dumps(config_data, indent=3))\n",
    "\n",
    "image_resize  = config_data['training']['image_resize']\n",
    "batch_size    = config_data['training']['batch_size']\n",
    "label_list    = config_data['dataset']['label_info']\n",
    "epochs        = config_data['training']['epochs']\n",
    "loss_name     = config_data['training']['loss']['name']\n",
    "early_stop    = config_data['training']['early_stopping']\n",
    "start_val_los = config_data['training']['start_val_loss']\n",
    "\n",
    "label_read    = json.load(open(label_list))\n",
    "species_list  = label_read['species_list']\n",
    "genus_list    = label_read['genus_list']\n",
    "family_list   = label_read['family_list']\n",
    "\n",
    "no_species_cl = config_data['model']['species_num_classes']\n",
    "no_genus_cl   = config_data['model']['genus_num_classes']\n",
    "no_family_cl  = config_data['model']['family_num_classes']\n",
    "\n",
    "opt_name      = config_data['training']['optimizer']['name']\n",
    "learning_rate = config_data['training']['optimizer']['learning_rate']\n",
    "momentum      = config_data['training']['optimizer']['momentum']\n",
    "\n",
    "mod_save_pth  = config_data['training']['model_save_path']\n",
    "mod_name      = config_data['training']['model_name']\n",
    "mod_ver       = config_data['training']['version']\n",
    "DTSTR         = datetime.datetime.now()\n",
    "DTSTR         = DTSTR.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "save_path     = mod_save_pth + mod_ver + '_' + mod_name + '_' + DTSTR + '.pt'\n",
    "\n",
    "taxon_hierar  = config_data['dataset']['taxon_hierarchy']\n",
    "label_info    = config_data['dataset']['label_info']\n",
    "\n",
    "# wandb.init(project=\"UK Moth Classifier\", entity=\"moth-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model = build_model(config_data).to(device)\n",
    "# print(model)\n",
    "# print(summary(model, (3,224,224)))  # keras-type model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = '/home/mila/a/aditya.jain/scratch/GBIF_Data/webdataset_moths_uk-denmark/train/train-500-000000.tar'\n",
    "val_url = '/home/mila/a/aditya.jain/scratch/GBIF_Data/webdataset_moths_uk-denmark/val/val-500-000000.tar'\n",
    "test_url = '/home/mila/a/aditya.jain/scratch/GBIF_Data/webdataset_moths_uk-denmark/test/test-500-000000.tar'\n",
    "num_workers = 1\n",
    "\n",
    "train_dataloader = dataloader.build_webdataset_pipeline(\n",
    "    sharedurl=train_url,\n",
    "    input_size=image_resize,\n",
    "    batch_size=64,\n",
    "    is_training=True,\n",
    "    num_workers=1,\n",
    "    preprocess_mode='tf')\n",
    "\n",
    "val_dataloader = dataloader.build_webdataset_pipeline(\n",
    "    sharedurl=val_url,\n",
    "    input_size=image_resize,\n",
    "    batch_size=batch_size,\n",
    "    is_training=False,\n",
    "    num_workers=1,\n",
    "    preprocess_mode='tf')\n",
    "\n",
    "\n",
    "test_dataloader = dataloader.build_webdataset_pipeline(\n",
    "    sharedurl=test_url,\n",
    "    input_size=image_resize,\n",
    "    batch_size=batch_size,\n",
    "    is_training=False,\n",
    "    num_workers=1,\n",
    "    preprocess_mode='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = Loss(loss_name).func()\n",
    "optimizer = Optimizer(opt_name, model, learning_rate, momentum).func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_particular_species_data(labels, outputs, species_list):\n",
    "    \"\"\"selects a particular set of species data from the batch\"\"\"\n",
    "    \n",
    "    species_list  = json.load(open(species_list))\n",
    "    index_list    = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in species_list:\n",
    "            index_list.append(True)\n",
    "        else:\n",
    "            index_list.append(False)\n",
    "            \n",
    "    return labels[index_list], outputs[index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_val_loss = start_val_los\n",
    "early_stp_count = 0\n",
    "part_sp_list    = '/home/mila/a/aditya.jain/gbif_species_trainer/model_training/data/uk-denmark_overfit_test_species.json'\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0\n",
    "    val_loss   = 0\n",
    "    s_time     = time.time()\n",
    "    \n",
    "    global_microacc_data_train = None\n",
    "    global_microacc_data_val   = None\n",
    "    \n",
    "    ## model training on training dataset\n",
    "    model.train()                      # switching model to training mode\n",
    "    for image_batch, label_batch in train_dataloader:    \n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)         \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs   = model(image_batch)\n",
    "        print('labels: ', label_batch.shape)\n",
    "        print('output: ', outputs.shape)\n",
    "        label_batch, outputs = select_particular_species_data(label_batch, outputs, part_sp_list)\n",
    "        print('labels: ', label_batch.shape)\n",
    "        print('output: ', outputs.shape)\n",
    "        t_loss    = loss_func(outputs, label_batch)\n",
    "        t_loss.backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += t_loss.item()\n",
    "        \n",
    "        # micro-accuracy calculation\n",
    "        micro_accuracy_train          = MicroAccuracyBatch(outputs, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "        global_microacc_data_train    = add_batch_microacc(global_microacc_data_train, micro_accuracy_train)\n",
    "        \n",
    "    ## model evaluation on validation dataset\n",
    "    model.eval()                       # switching model to evaluation mode\n",
    "    for image_batch, label_batch in val_dataloader:\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        label_batch              = label_batch.squeeze_()        \n",
    "        \n",
    "        outputs   = model(image_batch)        \n",
    "        v_loss    = loss_func(outputs, label_batch)\n",
    "        val_loss += v_loss.item()    \n",
    "        \n",
    "        # micro-accuracy calculation\n",
    "        micro_accuracy_val          = MicroAccuracyBatch(outputs, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "        global_microacc_data_val    = add_batch_microacc(global_microacc_data_val, micro_accuracy_val)\n",
    "    \n",
    "        if val_loss<lowest_val_loss:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss':val_loss}, \n",
    "                save_path)        \n",
    "            lowest_val_loss = val_loss\n",
    "            early_stp_count = 0\n",
    "        else:\n",
    "            early_stp_count += 1        \n",
    "\n",
    "    # logging metrics\n",
    "    wandb.log({'training loss': train_loss, 'validation loss': val_loss})\n",
    "    wandb.log({'training accuracy': (train_corr_pred/train_total_pred)*100,\\\n",
    "               'validation accuracy': (val_corr_pred/val_total_pred)*100}) \n",
    "    \n",
    "    final_micro_accuracy_train = final_microacc(global_microacc_data_train)\n",
    "    final_micro_accuracy_val   = final_microacc(global_microacc_data_val) \n",
    "    wandb.log({'train_micro_species_top1': final_micro_accuracy_train['micro_species_top1'], \n",
    "               'train_micro_genus_top1': final_micro_accuracy_train['micro_genus_top1'],\n",
    "               'train_micro_family_top1': final_micro_accuracy_train['micro_family_top1'],\n",
    "               'val_micro_species_top1': final_micro_accuracy_val['micro_species_top1'], \n",
    "               'val_micro_genus_top1': final_micro_accuracy_val['micro_genus_top1'],\n",
    "               'val_micro_family_top1': final_micro_accuracy_val['micro_family_top1']\n",
    "              })   \n",
    "    \n",
    "    e_time = (time.time()-s_time)/60   # time taken in minutes    \n",
    "    wandb.log({'time per epoch': e_time})\n",
    "    \n",
    "    if early_stp_count >= early_stop:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH       = '/home/mila/a/aditya.jain/logs/v01_mothmodel_2021-06-08-04-53.pt'\n",
    "checkpoint = torch.load(PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.load_state_dict(torch.load('logs/mothmodel_2021-05-18-07-36.pt', map_location=device))   #**** Wouldn't need this while running this as a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()                                          # putting the model in evaluation mode\n",
    "global_microacc_data     = None\n",
    "global_macroacc_data     = None\n",
    "global_confusion_data_sp = None\n",
    "global_confusion_data_g  = None\n",
    "global_confusion_data_f  = None\n",
    "\n",
    "print(\"Prediction on test data started ...\")\n",
    "\n",
    "with torch.no_grad():                                 # switching off gradient computation in evaluation mode\n",
    "    for image_batch, label_batch in test_dataloader:  \n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        predictions              = model(image_batch)\n",
    "        \n",
    "        # micro-accuracy calculation\n",
    "        micro_accuracy           = MicroAccuracyBatch(predictions, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "        global_microacc_data     = add_batch_microacc(global_microacc_data, micro_accuracy)\n",
    "        \n",
    "        # macro-accuracy calculation\n",
    "        macro_accuracy           = MacroAccuracyBatch(predictions, label_batch, label_info, taxon_hierar).batch_accuracy()\n",
    "        global_macroacc_data     = add_batch_macroacc(global_macroacc_data, macro_accuracy) \n",
    "        \n",
    "        # confusion matrix\n",
    "        sp_label_batch, sp_predictions, g_label_batch, g_predictions, f_label_batch, f_predictions = ConfusionDataConvert(predictions, label_batch, label_info, taxon_hierar).converted_data()   \n",
    "        \n",
    "        global_confusion_data_sp = confusion_matrix_data(global_confusion_data_sp, [sp_label_batch, sp_predictions])\n",
    "        global_confusion_data_g  = confusion_matrix_data(global_confusion_data_g, [g_label_batch, g_predictions])\n",
    "        global_confusion_data_f  = confusion_matrix_data(global_confusion_data_f, [f_label_batch, f_predictions])        \n",
    "    \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_micro_accuracy            = final_microacc(global_microacc_data)\n",
    "final_macro_accuracy, taxon_acc = final_macroacc(global_macroacc_data)\n",
    "tax_accuracy                    = taxon_accuracy(taxon_acc, label_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving evaluation data to file\n",
    "\n",
    "confdata_pd_f  = pd.DataFrame({'F_Truth': global_confusion_data_f[0].reshape(-1), 'F_Prediction': global_confusion_data_f[1].reshape(-1)})\n",
    "confdata_pd_g  = pd.DataFrame({'G_Truth': global_confusion_data_g[0].reshape(-1), 'G_Prediction': global_confusion_data_g[1].reshape(-1)})\n",
    "confdata_pd_sp = pd.DataFrame({'S_Truth': global_confusion_data_sp[0].reshape(-1), 'S_Prediction': global_confusion_data_sp[1].reshape(-1)})\n",
    "confdata_pd    = pd.concat([confdata_pd_f, confdata_pd_g, confdata_pd_sp], axis=1)\n",
    "\n",
    "confdata_pd.to_csv(mod_save_pth + mod_ver + '_confusion-data.csv', index=False)\n",
    "\n",
    "with open(mod_save_pth + mod_ver + '_micro-accuracy.json', 'w') as outfile:\n",
    "    json.dump(final_micro_accuracy, outfile)\n",
    "\n",
    "with open(mod_save_pth + mod_ver + '_macro-accuracy.json', 'w') as outfile:\n",
    "    json.dump(final_macro_accuracy, outfile)\n",
    "    \n",
    "with open(mod_save_pth + mod_ver + '_taxon-accuracy.json', 'w') as outfile:\n",
    "    json.dump(tax_accuracy, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'final micro accuracy' : final_micro_accuracy})\n",
    "wandb.log({'final macro accuracy' : final_macro_accuracy})\n",
    "wandb.log({'configuration' : config_data})\n",
    "wandb.log({'tax accuracy' : tax_accuracy})\n",
    "\n",
    "label_f = tf.keras.utils.to_categorical(global_confusion_data_f[0], num_classes=no_family_cl)\n",
    "pred_f  = tf.keras.utils.to_categorical(global_confusion_data_f[1], num_classes=no_family_cl)\n",
    "# experiment.log_confusion_matrix(label_f, pred_f, labels=family_list,\n",
    "#                                 max_example_per_cell=100000,\n",
    "#                                 title=\"Family Confusion Matrix\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor(\n",
    "    [[0.2215, 0.5859, 0.4782, 0.7411],\n",
    "    [0.3078, 0.3854, 0.3981, 0.5200],\n",
    "    [0.1363, 0.4060, 0.2030, 0.4940],\n",
    "    [0.1640, 0.6025, 0.2267, 0.7036],\n",
    "    [0.2445, 0.3032, 0.3300, 0.4253]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listt = [False, False, False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[listt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (milamoth)",
   "language": "python",
   "name": "milamoth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
