{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor       : Aditya Jain\\nDate Started : June 14, 2022\\nAbout        : This is the main training file for training the uk moth classifier\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author       : Aditya Jain\n",
    "Date Started : June 14, 2022\n",
    "About        : This is the main training file for training the uk moth classifier\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torchvision.models as torchmodels\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import webdataset as wds\n",
    "import sys \n",
    "\n",
    "from data import dataloader\n",
    "from models.build_model import build_model\n",
    "from data.mothdataset import MOTHDataset\n",
    "from training_params.loss import Loss\n",
    "from training_params.optimizer import Optimizer\n",
    "from evaluation.micro_accuracy_batch import MicroAccuracyBatch\n",
    "from evaluation.micro_accuracy_batch import add_batch_microacc, final_microacc\n",
    "from evaluation.macro_accuracy_batch import MacroAccuracyBatch\n",
    "from evaluation.macro_accuracy_batch import add_batch_macroacc, final_macroacc, taxon_accuracy\n",
    "from evaluation.confusion_matrix_data import confusion_matrix_data\n",
    "from evaluation.confusion_data_conversion import ConfusionDataConvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload files in `my_local_module`\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"note\": \"This config file is for training on the complete macromoths dataset\",\n",
      "   \"model\": {\n",
      "      \"species_num_classes\": 3175,\n",
      "      \"genus_num_classes\": 1088,\n",
      "      \"family_num_classes\": 71,\n",
      "      \"type\": \"efficientnetv2-b3\",\n",
      "      \"preprocess_mode\": \"tf\"\n",
      "   },\n",
      "   \"dataset\": {\n",
      "      \"label_info\": \"/home/mila/a/aditya.jain/gbif_species_trainer/model_training/data/uk-denmark_numeric_labels.json\",\n",
      "      \"taxon_hierarchy\": \"/home/mila/a/aditya.jain/gbif_species_trainer/model_training/data/uk-denmark_taxon_hierarchy.json\"\n",
      "   },\n",
      "   \"training\": {\n",
      "      \"batch_size\": 64,\n",
      "      \"image_resize\": 300,\n",
      "      \"epochs\": 30,\n",
      "      \"early_stopping\": 4,\n",
      "      \"start_val_loss\": 100000000,\n",
      "      \"loss\": {\n",
      "         \"name\": \"crossentropy\"\n",
      "      },\n",
      "      \"optimizer\": {\n",
      "         \"name\": \"sgd\",\n",
      "         \"learning_rate\": 0.001,\n",
      "         \"momentum\": 0.9\n",
      "      },\n",
      "      \"wandb\": {\n",
      "         \"entity\": \"moth-ai\",\n",
      "         \"project\": \"UK-Denmark-Moth-Classifier\"\n",
      "      },\n",
      "      \"model_save_path\": \"/home/mila/a/aditya.jain/logs/\",\n",
      "      \"model_name\": \"uk-denmark-moth-model\",\n",
      "      \"version\": \"v01\"\n",
      "   }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config_file   = 'config/01-config_uk-denmark.json' #**** This file path should come through command line argument\n",
    "\n",
    "f             = open(config_file)\n",
    "config_data   = json.load(f)\n",
    "print(json.dumps(config_data, indent=3))\n",
    "\n",
    "image_resize  = config_data['training']['image_resize']\n",
    "batch_size    = config_data['training']['batch_size']\n",
    "label_list    = config_data['dataset']['label_info']\n",
    "epochs        = config_data['training']['epochs']\n",
    "loss_name     = config_data['training']['loss']['name']\n",
    "early_stop    = config_data['training']['early_stopping']\n",
    "start_val_los = config_data['training']['start_val_loss']\n",
    "\n",
    "label_read    = json.load(open(label_list))\n",
    "species_list  = label_read['species_list']\n",
    "genus_list    = label_read['genus_list']\n",
    "family_list   = label_read['family_list']\n",
    "\n",
    "no_species_cl = config_data['model']['species_num_classes']\n",
    "no_genus_cl   = config_data['model']['genus_num_classes']\n",
    "no_family_cl  = config_data['model']['family_num_classes']\n",
    "\n",
    "opt_name      = config_data['training']['optimizer']['name']\n",
    "learning_rate = config_data['training']['optimizer']['learning_rate']\n",
    "momentum      = config_data['training']['optimizer']['momentum']\n",
    "\n",
    "mod_save_pth  = config_data['training']['model_save_path']\n",
    "mod_name      = config_data['training']['model_name']\n",
    "mod_ver       = config_data['training']['version']\n",
    "DTSTR         = datetime.datetime.now()\n",
    "DTSTR         = DTSTR.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "save_path     = mod_save_pth + mod_ver + '_' + mod_name + '_' + DTSTR + '.pt'\n",
    "\n",
    "taxon_hierar  = config_data['dataset']['taxon_hierarchy']\n",
    "label_info    = config_data['dataset']['label_info']\n",
    "\n",
    "# wandb.init(project=\"UK Moth Classifier\", entity=\"moth-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model = build_model(config_data).to(device)\n",
    "# print(model)\n",
    "# print(summary(model, (3,224,224)))  # keras-type model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = '/home/mila/a/aditya.jain/scratch/GBIF_Data/webdataset_moths_uk-denmark/train/train-500-000000.tar'\n",
    "val_url = '/home/mila/a/aditya.jain/scratch/GBIF_Data/webdataset_moths_uk-denmark/val/val-500-000000.tar'\n",
    "test_url = '/home/mila/a/aditya.jain/scratch/GBIF_Data/webdataset_moths_uk-denmark/test/test-500-000000.tar'\n",
    "num_workers = 1\n",
    "\n",
    "train_dataloader = dataloader.build_webdataset_pipeline(\n",
    "    sharedurl=train_url,\n",
    "    input_size=image_resize,\n",
    "    batch_size=64,\n",
    "    is_training=True,\n",
    "    num_workers=1,\n",
    "    preprocess_mode='tf')\n",
    "\n",
    "val_dataloader = dataloader.build_webdataset_pipeline(\n",
    "    sharedurl=val_url,\n",
    "    input_size=image_resize,\n",
    "    batch_size=batch_size,\n",
    "    is_training=False,\n",
    "    num_workers=1,\n",
    "    preprocess_mode='tf')\n",
    "\n",
    "\n",
    "test_dataloader = dataloader.build_webdataset_pipeline(\n",
    "    sharedurl=test_url,\n",
    "    input_size=image_resize,\n",
    "    batch_size=batch_size,\n",
    "    is_training=False,\n",
    "    num_workers=1,\n",
    "    preprocess_mode='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = Loss(loss_name).func()\n",
    "optimizer = Optimizer(opt_name, model, learning_rate, momentum).func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_particular_species_data(labels, outputs, species_list):\n",
    "    \"\"\"selects a particular set of species data from the batch\"\"\"\n",
    "    \n",
    "    species_list  = json.load(open(species_list))\n",
    "    index_list    = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] in species_list:\n",
    "            index_list.append(True)\n",
    "        else:\n",
    "            index_list.append(False)\n",
    "            \n",
    "    return labels[index_list], outputs[index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  tensor([1310, 1766,  611, 2806,    5,  819, 2606, 2828, 1566, 1491, 3168, 2719,\n",
      "        2216,  628, 2416, 1866, 1181, 2576, 1124, 1896, 1955, 1042,  620, 1599,\n",
      "        2354, 2220,  284, 2729, 2192,  396, 1566, 2629, 2663, 1712, 2126, 1838,\n",
      "         452,  229, 2674, 2864, 1156, 2400, 2268, 1561, 1307,  528,  561, 2806,\n",
      "        1709, 2732,   56, 1689, 1930, 3140, 1310, 3073, 2890,  967, 2105,  690,\n",
      "         446, 2806, 2266, 3001], device='cuda:0')\n",
      "output:  tensor([[ 0.1547,  0.0075,  0.0653,  ...,  0.0670, -0.1387, -0.0643],\n",
      "        [-0.0826,  0.0469, -0.0627,  ..., -0.1952,  0.0718, -0.1185],\n",
      "        [-0.0824,  0.1011,  0.0351,  ...,  0.0906, -0.1756, -0.0611],\n",
      "        ...,\n",
      "        [ 0.1060,  0.0319, -0.1030,  ...,  0.0056, -0.0911, -0.0272],\n",
      "        [ 0.0468,  0.0997, -0.0848,  ..., -0.0854, -0.0851,  0.0449],\n",
      "        [ 0.0078,  0.0260, -0.0973,  ..., -0.0732, -0.0659, -0.0348]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "labels:  tensor([1766,  611,  819, 1566, 2719, 2216,  628, 1866, 2576, 1124, 1896, 1042,\n",
      "         620, 2354, 2220, 2729,  396, 1566, 2629, 2663, 1712, 1838,  229, 1156,\n",
      "        2400, 1561,  561, 1709,   56, 1689, 1930,  967, 2105,  690, 2266],\n",
      "       device='cuda:0')\n",
      "output:  tensor([[-0.0826,  0.0469, -0.0627,  ..., -0.1952,  0.0718, -0.1185],\n",
      "        [-0.0824,  0.1011,  0.0351,  ...,  0.0906, -0.1756, -0.0611],\n",
      "        [-0.0665,  0.0737,  0.1123,  ...,  0.0783,  0.0754,  0.0123],\n",
      "        ...,\n",
      "        [-0.0620,  0.0562,  0.0582,  ...,  0.0204,  0.0740, -0.0237],\n",
      "        [ 0.0797,  0.0144,  0.1868,  ..., -0.1370, -0.0201,  0.0109],\n",
      "        [ 0.0468,  0.0997, -0.0848,  ..., -0.0854, -0.0851,  0.0449]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward>)\n",
      "labels:  tensor([1733, 1814, 2458,  221, 3149, 2870, 2583,  670, 2272, 2416, 2607,  229,\n",
      "        1060,  285,  350, 1636, 2726, 2672,  152, 1939, 2840, 2430, 2582, 2487,\n",
      "        1565, 3105, 3038,  190,  968, 1566, 2885, 1911, 1409,  987, 1926, 2067,\n",
      "        1417, 2778, 2755, 2062, 1409,  457, 1095, 1861,   80,  357, 1008, 1393,\n",
      "        3083, 1035, 2852, 2082, 2086,  617, 1907,  720,  273, 1984, 1959, 1135,\n",
      "        2019, 2431, 2419, 2718], device='cuda:0')\n",
      "output:  tensor([[-0.2328,  0.1462, -0.0908,  ..., -0.0996, -0.1564,  0.1815],\n",
      "        [ 0.0330, -0.0810,  0.1344,  ..., -0.0032, -0.1427, -0.1028],\n",
      "        [ 0.0119, -0.0681,  0.1810,  ...,  0.0097, -0.0300, -0.1523],\n",
      "        ...,\n",
      "        [ 0.0656, -0.0511,  0.0373,  ..., -0.0029, -0.0844, -0.0867],\n",
      "        [-0.1655,  0.0767, -0.0258,  ..., -0.0537,  0.0092, -0.0846],\n",
      "        [-0.1996,  0.0651, -0.0558,  ..., -0.1412,  0.0086,  0.1523]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "labels:  tensor([ 221, 3149, 2870, 2583,  670, 2607,  229, 1060,  285, 2726, 2672, 1939,\n",
      "        2582, 1565, 1566, 2885, 1409, 1926, 2067, 1417, 2778, 2755, 1409, 1861,\n",
      "          80,  357, 1035, 2082, 2086, 1907,  720,  273, 1984, 2431, 2419],\n",
      "       device='cuda:0')\n",
      "output:  tensor([[-0.0114, -0.0887, -0.1074,  ..., -0.0353,  0.0246, -0.0506],\n",
      "        [ 0.0052, -0.0049,  0.0155,  ...,  0.0247, -0.0318, -0.0250],\n",
      "        [-0.0190,  0.0219, -0.0040,  ..., -0.1531,  0.0438, -0.0232],\n",
      "        ...,\n",
      "        [-0.0106, -0.0074,  0.0251,  ...,  0.0665, -0.0275,  0.0179],\n",
      "        [ 0.0656, -0.0511,  0.0373,  ..., -0.0029, -0.0844, -0.0867],\n",
      "        [-0.1655,  0.0767, -0.0258,  ..., -0.0537,  0.0092, -0.0846]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward>)\n",
      "labels:  tensor([2701, 2119, 1724, 2830, 2367,  771, 1645, 2158,  663, 1604,  175,  229,\n",
      "        1218,  890, 1745, 2465,  836, 3109,  819, 2929,  592, 2114, 2897, 2105,\n",
      "        1920,  860, 2462,  480, 1067, 2885,  661, 2553, 1179, 3131, 2268, 2656,\n",
      "         682, 3104, 1153, 1872, 2203, 1178,  776, 2663, 2472, 2272, 1794,  262,\n",
      "        3027,  653, 3097,   46, 1496, 1885, 2628,  254, 1128, 2452, 1665, 1796,\n",
      "          41,  285, 1225,  793], device='cuda:0')\n",
      "output:  tensor([[ 0.0074,  0.1214, -0.0445,  ...,  0.1078,  0.1754, -0.1887],\n",
      "        [-0.1627,  0.1003, -0.1174,  ..., -0.0256, -0.0471, -0.0552],\n",
      "        [ 0.1157, -0.0114,  0.0556,  ...,  0.0367,  0.0177, -0.0411],\n",
      "        ...,\n",
      "        [-0.0230, -0.0950,  0.1077,  ..., -0.0351, -0.0419,  0.0562],\n",
      "        [-0.1337, -0.0361, -0.0287,  ..., -0.2589, -0.1476, -0.0502],\n",
      "        [-0.0951,  0.1622,  0.1529,  ...,  0.0275, -0.0323, -0.0081]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "labels:  tensor([2119, 2830, 2367,  771,  229, 1218, 2465,  836, 3109,  819, 2897, 2105,\n",
      "         860, 2885,  661, 2553, 1179, 3131, 1153, 1178,  776, 2663, 2472, 1794,\n",
      "          46, 2628,  254, 1128, 2452, 1796,  285, 1225,  793], device='cuda:0')\n",
      "output:  tensor([[-0.1627,  0.1003, -0.1174,  ..., -0.0256, -0.0471, -0.0552],\n",
      "        [ 0.0664,  0.1353, -0.0056,  ...,  0.0908, -0.0391, -0.0630],\n",
      "        [-0.1299, -0.1614, -0.1233,  ..., -0.0826,  0.0068, -0.0083],\n",
      "        ...,\n",
      "        [-0.0230, -0.0950,  0.1077,  ..., -0.0351, -0.0419,  0.0562],\n",
      "        [-0.1337, -0.0361, -0.0287,  ..., -0.2589, -0.1476, -0.0502],\n",
      "        [-0.0951,  0.1622,  0.1529,  ...,  0.0275, -0.0323, -0.0081]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward>)\n",
      "labels:  tensor([2797, 1848, 2605,  917, 3137, 1468, 2362, 2942, 1777,  628, 2233,   56,\n",
      "        1785, 2665, 2233, 2863, 1310, 2916, 1757, 1965, 1537, 2445,  156, 1949,\n",
      "         568, 2988,   57, 2469, 1669,  199, 2372, 2055,  649, 2841,  430, 2810,\n",
      "         725, 2181, 2040,  364, 1599, 3099, 2782, 1352, 3097, 2967,  451, 2925,\n",
      "         606, 2465, 2414, 1479, 2937, 2445, 1314, 1733,  805,  354,   18,  966,\n",
      "        2311,  747, 1175, 1364], device='cuda:0')\n",
      "output:  tensor([[-0.1845, -0.0513,  0.0487,  ..., -0.0550, -0.0628, -0.2128],\n",
      "        [ 0.0329, -0.0554, -0.0708,  ..., -0.1202, -0.1054, -0.1902],\n",
      "        [-0.0316, -0.0401,  0.0744,  ...,  0.0672, -0.0628,  0.0577],\n",
      "        ...,\n",
      "        [-0.0819, -0.1382, -0.0055,  ..., -0.0230, -0.0671, -0.0284],\n",
      "        [-0.0605,  0.0012,  0.0559,  ...,  0.0599, -0.0862,  0.0070],\n",
      "        [-0.0148,  0.0678,  0.1110,  ..., -0.0487, -0.0043, -0.1555]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3eac9f8d0730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_particular_species_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart_sp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2227c3161981>\u001b[0m in \u001b[0;36mselect_particular_species_data\u001b[0;34m(labels, outputs, species_list)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindex_list\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecies_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mindex_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/milamoth/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0massigned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRAPPER_ASSIGNMENTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massigned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lowest_val_loss = start_val_los\n",
    "early_stp_count = 0\n",
    "part_sp_list    = '/home/mila/a/aditya.jain/gbif_species_trainer/model_training/data/uk-denmark_overfit_test_species.json'\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0\n",
    "    val_loss   = 0\n",
    "    s_time     = time.time()\n",
    "    \n",
    "    global_microacc_data_train = None\n",
    "    global_microacc_data_val   = None\n",
    "    \n",
    "    ## model training on training dataset\n",
    "    model.train()                      # switching model to training mode\n",
    "    for image_batch, label_batch in train_dataloader:    \n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)         \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs   = model(image_batch)\n",
    "        print('labels: ', label_batch.shape)\n",
    "        print('output: ', outputs.shape)\n",
    "        label_batch, outputs = select_particular_species_data(label_batch, outputs, part_sp_list)\n",
    "        print('labels: ', label_batch.shape)\n",
    "        print('output: ', outputs.shape)\n",
    "        t_loss    = loss_func(outputs, label_batch)\n",
    "        t_loss.backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += t_loss.item()\n",
    "        \n",
    "        # micro-accuracy calculation\n",
    "        micro_accuracy_train          = MicroAccuracyBatch(outputs, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "        global_microacc_data_train    = add_batch_microacc(global_microacc_data_train, micro_accuracy_train)\n",
    "        \n",
    "    ## model evaluation on validation dataset\n",
    "    model.eval()                       # switching model to evaluation mode\n",
    "    for image_batch, label_batch in val_dataloader:\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        label_batch              = label_batch.squeeze_()        \n",
    "        \n",
    "        outputs   = model(image_batch)        \n",
    "        v_loss    = loss_func(outputs, label_batch)\n",
    "        val_loss += v_loss.item()    \n",
    "        \n",
    "        # micro-accuracy calculation\n",
    "        micro_accuracy_val          = MicroAccuracyBatch(outputs, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "        global_microacc_data_val    = add_batch_microacc(global_microacc_data_val, micro_accuracy_val)\n",
    "    \n",
    "        if val_loss<lowest_val_loss:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss':val_loss}, \n",
    "                save_path)        \n",
    "            lowest_val_loss = val_loss\n",
    "            early_stp_count = 0\n",
    "        else:\n",
    "            early_stp_count += 1        \n",
    "\n",
    "    # logging metrics\n",
    "    wandb.log({'training loss': train_loss, 'validation loss': val_loss})\n",
    "    wandb.log({'training accuracy': (train_corr_pred/train_total_pred)*100,\\\n",
    "               'validation accuracy': (val_corr_pred/val_total_pred)*100}) \n",
    "    \n",
    "    final_micro_accuracy_train = final_microacc(global_microacc_data_train)\n",
    "    final_micro_accuracy_val   = final_microacc(global_microacc_data_val) \n",
    "    wandb.log({'train_micro_species_top1': final_micro_accuracy_train['micro_species_top1'], \n",
    "               'train_micro_genus_top1': final_micro_accuracy_train['micro_genus_top1'],\n",
    "               'train_micro_family_top1': final_micro_accuracy_train['micro_family_top1'],\n",
    "               'val_micro_species_top1': final_micro_accuracy_val['micro_species_top1'], \n",
    "               'val_micro_genus_top1': final_micro_accuracy_val['micro_genus_top1'],\n",
    "               'val_micro_family_top1': final_micro_accuracy_val['micro_family_top1']\n",
    "              })   \n",
    "    \n",
    "    e_time = (time.time()-s_time)/60   # time taken in minutes    \n",
    "    wandb.log({'time per epoch': e_time})\n",
    "    \n",
    "    if early_stp_count >= early_stop:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH       = '/home/mila/a/aditya.jain/logs/v01_mothmodel_2021-06-08-04-53.pt'\n",
    "checkpoint = torch.load(PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.load_state_dict(torch.load('logs/mothmodel_2021-05-18-07-36.pt', map_location=device))   #**** Wouldn't need this while running this as a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()                                          # putting the model in evaluation mode\n",
    "global_microacc_data     = None\n",
    "global_macroacc_data     = None\n",
    "global_confusion_data_sp = None\n",
    "global_confusion_data_g  = None\n",
    "global_confusion_data_f  = None\n",
    "\n",
    "print(\"Prediction on test data started ...\")\n",
    "\n",
    "with torch.no_grad():                                 # switching off gradient computation in evaluation mode\n",
    "    for image_batch, label_batch in test_dataloader:  \n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        predictions              = model(image_batch)\n",
    "        \n",
    "        # micro-accuracy calculation\n",
    "        micro_accuracy           = MicroAccuracyBatch(predictions, label_batch, label_info, taxon_hierar).batch_accuracy()   \n",
    "        global_microacc_data     = add_batch_microacc(global_microacc_data, micro_accuracy)\n",
    "        \n",
    "        # macro-accuracy calculation\n",
    "        macro_accuracy           = MacroAccuracyBatch(predictions, label_batch, label_info, taxon_hierar).batch_accuracy()\n",
    "        global_macroacc_data     = add_batch_macroacc(global_macroacc_data, macro_accuracy) \n",
    "        \n",
    "        # confusion matrix\n",
    "        sp_label_batch, sp_predictions, g_label_batch, g_predictions, f_label_batch, f_predictions = ConfusionDataConvert(predictions, label_batch, label_info, taxon_hierar).converted_data()   \n",
    "        \n",
    "        global_confusion_data_sp = confusion_matrix_data(global_confusion_data_sp, [sp_label_batch, sp_predictions])\n",
    "        global_confusion_data_g  = confusion_matrix_data(global_confusion_data_g, [g_label_batch, g_predictions])\n",
    "        global_confusion_data_f  = confusion_matrix_data(global_confusion_data_f, [f_label_batch, f_predictions])        \n",
    "    \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_micro_accuracy            = final_microacc(global_microacc_data)\n",
    "final_macro_accuracy, taxon_acc = final_macroacc(global_macroacc_data)\n",
    "tax_accuracy                    = taxon_accuracy(taxon_acc, label_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving evaluation data to file\n",
    "\n",
    "confdata_pd_f  = pd.DataFrame({'F_Truth': global_confusion_data_f[0].reshape(-1), 'F_Prediction': global_confusion_data_f[1].reshape(-1)})\n",
    "confdata_pd_g  = pd.DataFrame({'G_Truth': global_confusion_data_g[0].reshape(-1), 'G_Prediction': global_confusion_data_g[1].reshape(-1)})\n",
    "confdata_pd_sp = pd.DataFrame({'S_Truth': global_confusion_data_sp[0].reshape(-1), 'S_Prediction': global_confusion_data_sp[1].reshape(-1)})\n",
    "confdata_pd    = pd.concat([confdata_pd_f, confdata_pd_g, confdata_pd_sp], axis=1)\n",
    "\n",
    "confdata_pd.to_csv(mod_save_pth + mod_ver + '_confusion-data.csv', index=False)\n",
    "\n",
    "with open(mod_save_pth + mod_ver + '_micro-accuracy.json', 'w') as outfile:\n",
    "    json.dump(final_micro_accuracy, outfile)\n",
    "\n",
    "with open(mod_save_pth + mod_ver + '_macro-accuracy.json', 'w') as outfile:\n",
    "    json.dump(final_macro_accuracy, outfile)\n",
    "    \n",
    "with open(mod_save_pth + mod_ver + '_taxon-accuracy.json', 'w') as outfile:\n",
    "    json.dump(tax_accuracy, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'final micro accuracy' : final_micro_accuracy})\n",
    "wandb.log({'final macro accuracy' : final_macro_accuracy})\n",
    "wandb.log({'configuration' : config_data})\n",
    "wandb.log({'tax accuracy' : tax_accuracy})\n",
    "\n",
    "label_f = tf.keras.utils.to_categorical(global_confusion_data_f[0], num_classes=no_family_cl)\n",
    "pred_f  = tf.keras.utils.to_categorical(global_confusion_data_f[1], num_classes=no_family_cl)\n",
    "# experiment.log_confusion_matrix(label_f, pred_f, labels=family_list,\n",
    "#                                 max_example_per_cell=100000,\n",
    "#                                 title=\"Family Confusion Matrix\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor(\n",
    "    [[0.2215, 0.5859, 0.4782, 0.7411],\n",
    "    [0.3078, 0.3854, 0.3981, 0.5200],\n",
    "    [0.1363, 0.4060, 0.2030, 0.4940],\n",
    "    [0.1640, 0.6025, 0.2267, 0.7036],\n",
    "    [0.2445, 0.3032, 0.3300, 0.4253]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "listt = [True, True, False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False, False,  True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,0]>0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bf8c8cdcdba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "a[:,0] in listt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2215, 0.5859, 0.4782, 0.7411],\n",
       "        [0.3078, 0.3854, 0.3981, 0.5200],\n",
       "        [0.1640, 0.6025, 0.2267, 0.7036]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[listt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (milamoth)",
   "language": "python",
   "name": "milamoth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
